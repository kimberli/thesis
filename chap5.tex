\chapter{Application: font glyph generation}\label{chap:training} To demonstrate our end-to-end SVG generation method, we train the model architecture described in Chapter~\ref{chap:architecture} on a dataset of font glyphs.

\section{Dataset}\label{sec:font-data}
Our dataset of font faces is downloaded from a GitHub repository containing all fonts available on Google Fonts (\url{https://github.com/google/fonts}).
The dataset contains 2552 font faces total from 877 font families.
Font faces exhibit much stylistic variation: a breakdown of font types for the 877 font families can be found in Table~\ref{tbl:fonttypes}, and examples of font glyphs from each type can be found in Figure~\ref{fig:fonttypes}.

\begin{table}[h]
\centering
\caption[A breakdown of font types in the Google Fonts dataset]
    {The 2552 font faces in the Google Fonts dataset are from 877 total font families.
    Each font family can be classified as one of the following categories, with the ``display'' category encompassing bold fonts of a wide variety of styles that can be used as title text.
    Here, we report the counts of font families in the dataset belonging to each type.\label{tbl:fonttypes}}
\begin{tabular}{c c c c c}
\toprule
    Serif & Sans serif & Display & Handwriting & Monospace \\ \midrule
    180 & 249 & 296 & 135 & 17
\end{tabular}
\end{table}

All font faces are downloaded as TTF files then converted to SVG format using the FontForge command-line tool\footnote{\url{https://fontforge.github.io}}.

The statistics for the glyph \textbf{b} across all font faces are shown in Figure~\ref{fig:stats-b}.
The remaining glyph statistics can be found in Appendix~\ref{app:data}.

\begin{figure}[h]
    \caption[Dataset statistics for the glyph \textbf{b}]
    {Dataset statistics for the glyph \textbf{b} across all 2552 font faces in the Google Fonts dataset.\label{fig:stats-b}}
\centering
\subcaptionbox*{}
{\begin{tikzpicture}
\begin{axis}[
 	xlabel={feature vector count},
    ylabel={number of \textbf{b} glyphs},
	width=0.45\textwidth,
    ybar,
    ymin=0
]
\addplot [
	fill=color1,
	fill opacity=0.4,
    hist={
        bins=7,
        data min=0.5,
        data max=300
    }   
] table [y index=0] {data/b_points_stats.csv};
\end{axis}
\end{tikzpicture}
}
\subcaptionbox*{}
{\begin{tikzpicture}
\begin{axis}[
 	xlabel={command type count},
    ylabel={number of \textbf{b} glyphs},
	width=0.45\textwidth,
    ybar,
    ymin=0
]
\addplot [
	fill=color2,
	fill opacity=0.4,
    hist={
        bins=7,
        data min=0.5,
        data max=100
    }   
] table [y index=0] {data/b_line_count.csv};
\addplot [
	fill=color3,
	fill opacity=0.4,
    hist={
        bins=7,
        data min=0.5,
        data max=100
    }   
] table [y index=0] {data/b_quad_count.csv};
\addlegendentry{\code{line}}
\addlegendentry{\code{quad}}
\end{axis}
\end{tikzpicture}
}
\end{figure}

\section{Training}
We train the model on single-class datasets for the glyphs \textbf{b}, \textbf{f}, \textbf{g}, \textbf{o}, and \textbf{x}, chosen because they cover a variety of shapes and curves.
Each input SVG is transformed to a list of feature vectors using the method described in Chapter~\ref{chap:features}.
We use encoding B, details of which are described in Chapter~\ref{chap:feature-variation}.
The datasets are then randomly partitioned into training, test, and validation sets; glyphs from the overall dataset of 2552 font faces are pruned if they consist of more than 250 feature vectors.
Every glyph except \textbf{x} has a training set of 1920 SVGs, while \textbf{x} has a training set of 1960.
All glyphs have test and validation sets of 240 SVGs each.
Training is done with a batch size of 40, a learning rate of 0.001, and a KL weight in the loss function that starts at 0.01 and increases slowly over time.
The size of the latent space vector $z$ is set to 128, and we use recurrent dropout to reduce overfitting.
Cost graphs and details about trained models can be found in Appendix~\ref{app:train}.

\section{Results}
Here, we report quantitative results as well as a qualitative evaluation of model performance.
\begin{figure}[h]
    \centering
	\includegraphics[width=\textwidth]{figures/font_gen}
    \caption[Visual results of training single-class model on letter glyph datasets]
    {Selected glyphs conditionally generated by the trained single-class model.
    Ground truth inputs on the right are fed into the encoder and decoded into the generated outputs on the left.\label{fig:font_gen}}
\end{figure}

\subsection{Quantitative results}\label{sec:quant-eval}
To quantify model performance, we compute an image similarity metric between each ground-truth image and a corresponding image conditionally generated by the model with $\tau = 0.3$, where temperature $\tau$ increases randomness when sampling from the decoder as defined in~\cite{ha2017neural}.
Images are first converted to point clouds containing every pixel in the raster image with nonzero value.
The resulting sets of points are translated to be mean-centered for each image, and the modified Hausdorff distance is calculated from the point set of each generated image to the point set of its corresponding ground truth image~\cite{dubuisson1994modified}.
While we also considered using metrics such as pixel loss and feature extraction, we chose the Hausdorff distance for its simplicity as a measure of mutual polygonal proximity.
Evaluation is run on a set of $N$ test images for each glyph, and quantitative results can be found in Table~\ref{tbl:model-results}.

\begin{table}[h]
\centering
\caption[Quantitative results for models trained on letter glyphs]
    {Modified Hausdorff distance for models trained on each glyph on a test set of $N$ images.\label{tbl:model-results}}
\begin{tabular}{c c c c c}
\toprule
    Glyph & Mean & Std.\ dev. & Kurtosis & $N$ \\ \midrule
    \textbf{b} & 19.5341 & 8.3484 & 1.9515 & 240 \\
    \textbf{f} & 15.7533 & 9.2132 & 4.7359 & 240 \\
    \textbf{g} & 19.7714 & 9.6469 & 6.8383 & 240 \\
    \textbf{o} & 27.8857 & 11.9887 & 5.8081 & 240 \\
    \textbf{x} & 28.8542 & 14.2474 & 1.9820 & 238 
\end{tabular}
\end{table}

\subsection{Qualitative results}
We demonstrate the model's learned ability with a few illustrative examples.

In Figure~\ref{fig:interp}, we interpolate between latent vectors for input \textbf{b} and \textbf{f} glyphs of different styles.
If interpolated latent vectors tend to produce coherent outputs, it indicates that the latent space is efficiently used and that the KL regularization term in the loss function has sufficient power.
\begin{figure}[h]
    \centering
	\includegraphics[width=\textwidth]{figures/interp}
    \caption[Latent space interpolation for the single-class model]
    {The latent space abstractly encodes both style and method---it must represent both how to draw a given glyph as well as the identify different types of glyphs.
    The input glyphs are on the far sides of the figure, and we spherically interpolate between their latent vectors, decoding with $\tau=0.1$.\label{fig:interp}}
\end{figure}

In Figure~\ref{fig:temp}, we demonstrate how temperature $\tau$ affects the decoding process as a scaling factor to $\sigma$ in the sampled GMMs. Intuitively, a lower temperature indicates that the decoder samples outputs that are believes are more likely.
\begin{figure}[h]
    \centering
	\includegraphics[width=\textwidth]{figures/temp_grid}
    \caption[The temperature grid for a conditionally generated glyph]
    {To demonstrate how temperature affects decoding, we decode the same latent vector at different temperature settings.
    At the far left, $\tau=0.1$, and at the far right, $\tau=1.0$, with $\tau$ increasing by 0.1 for every intermediate image.\label{fig:temp}}
\end{figure}

Figure~\ref{fig:fails} depicts common failure modes in conditional generation.
Because the model is drawing sequences of commands moving to relative points, it often struggles with closing the loop and returning to its original path start point.
Additionally, since we some font face styles are exceptionally rare, the model fails to learn how represent non-standard font styles.
\begin{figure}[h]
    \centering
	\includegraphics[width=\textwidth]{figures/fails}
    \caption[Common failure cases for conditional generation]
    {The model generally makes a set of common mistakes, as shown here.
    It struggles with positioning of disconnected components (1), closing paths (2, 4), understanding uncommon styles (3, 5, 6), and sometimes confuses styles (7).\label{fig:fails}}
\end{figure}

Finally, Figure~\ref{fig:uncond} demonstrates the creative ability of the model: generating unseen examples of a variety of styles.
Instead of encoding an input SVG and passing in the resulting latent vector into the decoder, we train the decoder separately to learn unconditional generation without a $z$ input.
\begin{figure}[h]
    \centering
	\includegraphics[width=\textwidth]{figures/uncond}
    \caption[Unconditional generation for the single-class model on letter glyphs]
    {We generate novel, unseen examples by training the decoder only without any latent variable input, feeding only the previously generated command into the decoder cells.
    Decoding here is done at $\tau=0.01$.
    \label{fig:uncond}}
\end{figure}
