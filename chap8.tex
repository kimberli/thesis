\chapter{Conclusion}
Through this work, we demonstrate the viability of a generalized vector graphics generation method by modeling SVGs as sequences of drawing commands.
Inspired by~\cite{ha2017neural}, our model extends the sequence-to-sequence variational autoencoder approach to model curve control points as well as pen drawing points, producing parameterized B\'ezier curves as output.

We train the model on font glyphs, first establishing single-class models of selected character glyphs.
Next, we investigate feature encodings' effects on model performance, quantified using an image similarity metric on raster image output.
Finally, we explore adjustments to explicitly encode style and content separately in the architecture and train on a multi-class set of digit glyphs.

Although this work is preliminary, it sets the groundwork for future development of creative tools for designers.
By avoiding explicit parameterizations for vector graphics images, we build a framework for future generalizable tools that propose novel first draft designs.
